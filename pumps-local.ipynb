{"cells":[{"cell_type":"markdown","source":["<center><h1>PART 1: TRAIN A MODEL ON DATABRICKS NOTEBOOK</h1></center>\n<br>\nIn this notebook, we are going cover the end-to-end development of a non-trivial machine earning use-case in Databricks using Scikit-Learn\n\n#### ABOUT THE MODEL & DATA\n\nUsing data from Taarifa and the Tanzanian Ministry of Water, we will predict which pumps are functional, which need some repairs, and which don't work at all. The labels encompass three classes and the training data is based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, portable water is available to communities across Tanzania. This competition is hosted on [Driven Data.](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/)"],"metadata":{}},{"cell_type":"code","source":["from __future__ import print_function\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nimport category_encoders as ce\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom IPython.display import display"],"metadata":{"hiddenCell":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/pythonVirtualEnvDirs/virtualEnv-5c452274-6455-4542-89ca-079c13326f3c/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-5c452274-6455-4542-89ca-079c13326f3c/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["### ABOUT THE DATABRICKS ENVIRONMENT\n\nDatabricks clusters can be permanent or ephemeral. To store data, the best practice however, is to use Databricks file system (DBFS) - a persistent distributed storage that sits on top of Azure Blob Storage. Let's do some house keeping and create a user folder that can be used to store things like picked objects.\n\nFor local disk storage while development we will use the Databricks FileStore folder. `/FileStore` is a special folder within DBFS where you can save files and also download files to your local machine via a browser.  \nUse the Databricks Data menu/UI to upload the pumps_data.csv and new_pumps_data.csv to `/FileStore/tables/pumps` directory  \n\n```\n/FileStore\n  ├── tables                     -> Databricks by default stores data here\n  │   └──pumps                   -> we will create this project specific folder\n  │      ├── new_pumps_data.csv  -> scoring dataset with no labels\n  │      └── pumps_data.csv      -> training dataset with labels\n  └── users/jason/pumps          -> we will create this folder as our project root folder\n      ├───models                 -> store all pickle files\n      │    ├──  local            -> pickle files created by training locally in the notebook\n      │    ├──  rf.pkl           -> Random Forest estimator trained on AMLS\n      │    ├──  le.pkl           -> Preprocessing transformer trained on AMLS\n      │    ├──  ohc.pkl          -> Preprocessing transformer trained on AMLS\n      │    ├──  y_le.pkl         -> Preprocessing transformer trained on AMLS\n      └── scripts                -> scripts such as train.py and score.py for AMLS\n```\n\nLet's create a `Config` class to hold all the pertinent configurations and storage locations."],"metadata":{}},{"cell_type":"code","source":["class Config(object):\n\n    # define DBFS paths for sub-directories\n    PROJECT_DIR = '/FileStore/users/jason/pumps/'   # to be used largely by dbutils only\n    # note pure Python does not understand 'dbfs:/' so you need to use '/dbfs' in specifying the FileStore folder\n    MODELS_DIR = '/dbfs'+PROJECT_DIR+'models/local/'\n    SCRIPTS_DIR = '/dbfs'+PROJECT_DIR+'scripts/'\n    \n    # set location for uploading data\n    # default location for data is /FileStore/tables but we will use a pumps sub-directory\n    DATA_DIR = '/dbfs/FileStore/tables/pumps/'  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# create the project directories in FileStore if not already exists\ndbutils.fs.mkdirs(Config.PROJECT_DIR+'models/local/')\ndbutils.fs.mkdirs(Config.PROJECT_DIR+'scripts')\n# verify\ndbutils.fs.ls(Config.PROJECT_DIR)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>\n[FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/&apos;, name=&apos;models/&apos;, size=0),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/scripts/&apos;, name=&apos;scripts/&apos;, size=0)]\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### IMPORT DATA"],"metadata":{}},{"cell_type":"markdown","source":["Let's start by defining some useful helper functions."],"metadata":{}},{"cell_type":"code","source":["def print_nans(df):\n\n    print('Checking for NANs:............................')\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns={0: 'Missing Values', 1: '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n    print(\"Your selected dataframe has \" +\n          str(df.shape[1]) +\n          \" columns and \" +\n          str(len(df)) +\n          \" rows \\n\" \"There are \" +\n          str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n    print('..............................................')\n    return mis_val_table_ren_columns\n\n\ndef data_frame_imputer(df):\n    fill = pd.Series([df[c].value_counts().index[0]\n                      if df[c].dtype == np.dtype('O') else df[c].mean() for c in df],\n                     index=df.columns)\n    return df.fillna(fill)\n\n\ndef replace_with_grouped_mean(df, value, column, to_groupby):\n\n    invalid_mask = (df[column] == value)\n\n    # get the mean without the invalid value\n    means_by_group = (df[~invalid_mask].groupby(to_groupby)[column].mean())\n\n    # get an array of the means for all of the data\n    means_array = means_by_group[df[to_groupby].values].values\n\n    # assign the invalid values to means\n    df.loc[invalid_mask, column] = means_array[invalid_mask]\n\n    return df\n\n\ndef log_transformer(df, base, c=1):\n\n    if base == 'e' or base == np.e:\n        log = np.log\n\n    elif base == '10' or base == 10:\n        log = np.log10\n\n    else:\n        def log(x): return np.log(x) / np.log(base)\n\n    c = c\n    out = pd.DataFrame()\n    for _ in df:\n        out = df.apply(lambda x: log(x + c))\n    return out\n\n\ndef stratified_split(x, y, test_size):\n\n    from sklearn.model_selection import StratifiedShuffleSplit\n\n    sss = StratifiedShuffleSplit(n_splits=10, test_size=test_size, random_state=5)\n    sss.get_n_splits(x, y)\n    data_train = pd.DataFrame()\n    data_test = pd.DataFrame()\n    label_train = pd.DataFrame()\n    label_test = pd.DataFrame()\n    for train_index, test_index in sss.split(x, y):\n        data_train, data_test = x.iloc[train_index], x.iloc[test_index]\n        label_train, label_test = y.iloc[train_index], y.iloc[test_index]\n    return data_train, data_test, label_train, label_test\n"],"metadata":{"hiddenCell":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["def create_dataframe(x):\n    \"\"\"\n    Imports the pumps csv data file directly from DBFS.\n\n    :param x: full DBFS path to file\n              e.g: '/dbfs/FileStore/tables/pumps/pumps_data.csv'\n    :return: two dataframes that split data and labels\n    \"\"\"\n    # import raw data\n    raw = pd.read_csv(x, index_col=0)\n    labels = pd.DataFrame(raw['status_group'])\n    data = raw.drop('status_group', axis=1)\n    \n    return data, labels"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# check if data was uploaded successfully\ndata, labels = create_dataframe(x=Config.DATA_DIR+'pumps_data.csv')\ndata.head(2), labels.head(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>\n(       amount_tsh date_recorded  ...     waterpoint_type  waterpoint_type_group\n id                               ...                                           \n 69572      6000.0    2011-03-14  ...  communal standpipe     communal standpipe\n 8776          0.0    2013-03-06  ...  communal standpipe     communal standpipe\n \n [2 rows x 39 columns],       status_group\n id                \n 69572   functional\n 8776    functional)\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["### CLEAN DATA\n\nIn this step we'll clean the data, impute NANs etc. by invoking those helper functions."],"metadata":{}},{"cell_type":"code","source":["def clean_data(x, y):\n    \"\"\"\n    Takes the pumps data and label dataframe and cleans it\n\n    :param x: the pumps dataframe\n    :param y: the pumps labels dataframe\n    :return:  stratified splits for train and test\n    \"\"\"\n  \n    useful_columns = ['amount_tsh',\n                      'gps_height',\n                      'longitude',\n                      'latitude',\n                      'region',\n                      'population',\n                      'construction_year',\n                      'extraction_type_class',\n                      'management_group',\n                      'quality_group',\n                      'source_type',\n                      'waterpoint_type']\n\n    # subset to columns we care about\n    x = x[useful_columns]\n\n    # for column construction_year, values <=1000 are probably bad\n    invalid_rows = x['construction_year'] < 1000\n    valid_mean = int(x.construction_year[~invalid_rows].mean())\n    x.loc[invalid_rows, \"construction_year\"] = valid_mean\n\n    # in some columns 0 is an invalid value\n    x = replace_with_grouped_mean(df=x, value=0, column='longitude', to_groupby='region')\n    x = replace_with_grouped_mean(df=x, value=0, column='population', to_groupby='region')\n\n    # set latitude to the proper value\n    x = replace_with_grouped_mean(df=x, value=-2e-8, column='latitude', to_groupby='region')\n\n    # set amount_to non-zeroes\n    x = replace_with_grouped_mean(df=x, value=0, column='amount_tsh', to_groupby='region')\n\n    # remove na's\n    x = data_frame_imputer(df=x)\n\n    # print nans in the dataframe if any\n    print_nans(x)\n\n    # log transform numerical columns\n    num_cols = ['amount_tsh', 'population']\n    x[num_cols] = log_transformer(df=x[num_cols], base='e', c=1)\n\n    # do train/test split\n    x_train, x_test, y_train, y_test = stratified_split(x=x, y=y, test_size=0.2)\n\n    return x_train, x_test, y_train, y_test"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Before we proceed let's check our work"],"metadata":{}},{"cell_type":"code","source":["x_train, x_test, y_train, y_test = clean_data(data, labels)\ndisplay(x_train.head(2), y_train.head(2), x_test.head(2), y_test.head(2))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\nChecking for NANs:............................\nYour selected dataframe has 12 columns and 59400 rows \nThere are 0 columns that have missing values.\n..............................................\n       amount_tsh  gps_height  ...   source_type  waterpoint_type\nid                             ...                               \n28058    6.713211           0  ...           dam            other\n73541    8.228889           0  ...  shallow well        hand pump\n\n[2 rows x 12 columns]\n      status_group\nid                \n28058   functional\n73541   functional\n       amount_tsh  gps_height  ...   source_type     waterpoint_type\nid                             ...                                  \n2244     7.170629           0  ...    river/lake  communal standpipe\n21235    7.170629           0  ...  shallow well           hand pump\n\n[2 rows x 12 columns]\n         status_group\nid                   \n2244   non functional\n21235      functional\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### PREPROCESS DATA\n\nIn this step we'll label and one hot encode all categoricals"],"metadata":{}},{"cell_type":"code","source":["def train_pre_processing(x, y):\n\n    \"\"\"\n    Preprocesses the pumps train datasets by applying label\n    and one-hot encoding\n\n    :param x: the pumps x_train dataset\n    :param y: the upmps y_train dataset\n    :return: encoded datasets and the fitted transformers\n    \"\"\"\n\n    # transform categorical variables with encoders\n    le_cols = ['region']\n    ohc_cols = ['extraction_type_class',\n                'management_group',\n                'quality_group',\n                'source_type',\n                'waterpoint_type']\n\n    # define encoders include label encoding for the actual labels\n    # using handle_unknown='ignore' will leave out new unseen values so keep\n    # monitoring your data for changes\n    \n    le = ce.OrdinalEncoder(cols=le_cols,\n                           return_df=True,\n                           handle_unknown='ignore')\n\n    ohc = ce.OneHotEncoder(cols=ohc_cols,\n                           return_df=True,\n                           use_cat_names=False,\n                           handle_unknown='ignore')\n\n    y_le = ce.OrdinalEncoder(return_df=True,\n                             handle_unknown='ignore')\n\n    print(\"x_train shape: \", x.shape)\n    print(\"y_train shape: \", y.shape)\n    # apply the encoders\n    print(\"Running label and one-hot encoding on the train data...\")\n    x = le.fit_transform(x)\n    x = ohc.fit_transform(x)\n    y = y_le.fit_transform(y)\n    # update the transformers\n    le = le\n    ohc = ohc\n    y_le = y_le\n    print(\"Final x_train shape: \", x.shape)\n    print(\"Final y_train shape: \", y.shape)\n    print(\"done.\")\n    \n    return x, y, le, ohc, y_le\n\n\ndef test_pre_processing(x, y, le, ohc, y_le):\n\n    \"\"\"\n    Preprocesses the pumps test datasets by applying label\n    and one-hot encoding\n\n    :param x: the x_test dataset\n    :param y: the y_test dataset\n    :param le: the label encoder fitted from the train_pre_processing() function\n    :param ohc: the one-hot encoder fitted from the train_pre_processing() function\n    :param y_le: the y label encoder fitted from the train_pre_processing() function\n    :return: encoded x_test and y_test\n    \"\"\"\n  \n    print(\"x_test shape: \", x.shape)\n    print(\"y_test shape: \", y.shape)\n    print(\"Running label and one-hot encoding on the test data...\")\n    x = le.transform(x)\n    x = ohc.transform(x)\n    y = y_le.transform(y)\n    print(\"New x_test shape: \", x.shape)\n    print(\"New y_test shape: \", y.shape)\n    print(\"done.\")\n\n    return x, y\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Once again, let's check our work to ensure all categoricals are properly encoded"],"metadata":{}},{"cell_type":"code","source":["# pre-process training data\nx_train, y_train, le, ohc, y_le = train_pre_processing(x=x_train, y=y_train)\n# pre-process test data\nx_test, y_test = test_pre_processing(x=x_test, y=y_test, le=le, ohc=ohc, y_le=y_le)\ndisplay(x_train.head(2), y_train.head(2), x_test.head(2), y_test.head(2))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x_train shape:  (47520, 12)\ny_train shape:  (47520, 1)\nRunning label and one-hot encoding on the train data...\nFinal x_train shape:  (47520, 39)\nFinal y_train shape:  (47520, 1)\ndone.\nx_test shape:  (11880, 12)\ny_test shape:  (11880, 1)\nRunning label and one-hot encoding on the test data...\nNew x_test shape:  (11880, 39)\nNew y_test shape:  (11880, 1)\ndone.\n       extraction_type_class_1  ...  construction_year\nid                              ...                   \n28058                        1  ...               2008\n73541                        0  ...               1996\n\n[2 rows x 39 columns]\n       status_group\nid                 \n28058             1\n73541             1\n       extraction_type_class_1  ...  construction_year\nid                              ...                   \n2244                         0  ...               1996\n21235                        0  ...               1996\n\n[2 rows x 39 columns]\n       status_group\nid                 \n2244              3\n21235             1\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### TRAIN MODEL\n\nNow, we are going to create one function that will encompass all the flows all the way from importing csv to processing and model training."],"metadata":{}},{"cell_type":"code","source":["def train_and_evaluate(x, n_estimators=100, criterion='entropy', class_weight='balanced_subsample'):\n    \"\"\"\n    A full pipeline that cleans, preprocesses data and then fits a\n    random forest classifier\n\n    :param x              : full DBFS path to file\n                            e.g: '/dbfs/FileStore/tables/pumps/pumps_data.csv'\n    :param n_estimators:  : random forest parameter for number of branches\n    :param criterion:     : random forest parameter for node splitting methodology\n    :param class_weight:  : random forest parameter whether to treat all classes as balanced\n    :return               :  all estimator/transfromer objects and the accuracy metric\n    \"\"\"\n\n    # ingest and process data\n    data, labels = create_dataframe(x=x)\n    x_train, x_test, y_train, y_test = clean_data(x=data, y=labels)\n    x_train, y_train, le, ohc, y_le = train_pre_processing(x=x_train, y=y_train)\n    x_test, y_test = test_pre_processing(x=x_test, y=y_test, le=le, ohc=ohc, y_le=y_le)\n    \n    # train classifier\n    print(\"training classifier...\")\n    rf = RandomForestClassifier(n_estimators=n_estimators,\n                                criterion=criterion,\n                                class_weight=class_weight)\n    rf.fit(x_train, np.ravel(y_train))\n    print(\" classifier has been trained\")\n    \n    # evaluate on test set\n    test_pred = rf.predict(x_test)\n    accuracy = accuracy_score(y_test, test_pred)\n    print(\"Test Accuracy: \", accuracy)\n    \n    # we need to return the transformers also so that it gets captured in the global scope\n    # this will allow us to pass these objects to the save_model() function\n    return rf, accuracy, le, ohc, y_le\n  \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["Let's run the entire flow using the `train_and_evaluate()` function"],"metadata":{}},{"cell_type":"code","source":["rf, accuracy, le, ohc, y_le = train_and_evaluate(Config.DATA_DIR+'pumps_data.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\nChecking for NANs:............................\nYour selected dataframe has 12 columns and 59400 rows \nThere are 0 columns that have missing values.\n..............................................\nx_train shape:  (47520, 12)\ny_train shape:  (47520, 1)\nRunning label and one-hot encoding on the train data...\nFinal x_train shape:  (47520, 39)\nFinal y_train shape:  (47520, 1)\ndone.\nx_test shape:  (11880, 12)\ny_test shape:  (11880, 1)\nRunning label and one-hot encoding on the test data...\nNew x_test shape:  (11880, 39)\nNew y_test shape:  (11880, 1)\ndone.\ntraining classifier...\n classifier has been trained\nTest Accuracy:  0.7630471380471381\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["At this point. we're going to set up a function to export our pickled estimator and transformers to GCS"],"metadata":{}},{"cell_type":"code","source":["def save_model(estimator, dbfspath, file_name):\n\n    \"\"\"\n    :param estimator : estimator or transformer currently in memory to pickle\n    :param dbfspath  : path to DBFS directory to store file as expected by dbutils\n                       e.g: /FileStore/users/jason/pumps/models/\n    :param file_name : name of the pickled file\n    \"\"\"\n    \n    # dump pickle to local filesystem\n    joblib.dump(estimator, file_name) \n    # move the pickled object from the local folder on the cluster to DBFS\n    dbutils.fs.mv('file:/databricks/driver/'+file_name, dbfspath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["Let's test out the exporting function"],"metadata":{}},{"cell_type":"code","source":["save_model(rf, Config.PROJECT_DIR+'models/local/', file_name='rf.pkl')\nsave_model(le, Config.PROJECT_DIR+'models/local/', file_name='le.pkl')\nsave_model(ohc, Config.PROJECT_DIR+'models/local/', file_name='ohc.pkl')\nsave_model(y_le, Config.PROJECT_DIR+'models/local/', file_name='y_le.pkl')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["The pickled objects should now appear in DBFS"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.ls(Config.PROJECT_DIR+'models/local')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">17</span><span class=\"ansired\">]: </span>\n[FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/local/le.pkl&apos;, name=&apos;le.pkl&apos;, size=660),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/local/ohc.pkl&apos;, name=&apos;ohc.pkl&apos;, size=1237),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/local/rf.pkl&apos;, name=&apos;rf.pkl&apos;, size=162098007),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/local/y_le.pkl&apos;, name=&apos;y_le.pkl&apos;, size=359)]\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["### SCORE NEW DATA\n\nThe `process_data()` function will clean and preprocess any new data and output a format that the random forest model expects to see"],"metadata":{}},{"cell_type":"code","source":["def process_data(x, le, ohc):\n    \"\"\"\n    Gets new data ready for scoring\n\n    :param x: new data in the form of a dataframe\n    :param le: the pumps pickled label encoder transformer\n    :param ohc: the pumps pickled one-hot encoding transformer\n    :return:  dataframe ready for prediction\n    \"\"\"\n  \n    useful_columns = ['amount_tsh',\n                      'gps_height',\n                      'longitude',\n                      'latitude',\n                      'region',\n                      'population',\n                      'construction_year',\n                      'extraction_type_class',\n                      'management_group',\n                      'quality_group',\n                      'source_type',\n                      'waterpoint_type']\n\n    # subset to columns we care about\n    x = x[useful_columns]\n\n    # for column construction_year, values <=1000 are probably bad\n    invalid_rows = x['construction_year'] < 1000\n    valid_mean = int(x.construction_year[~invalid_rows].mean())\n    x.loc[invalid_rows, \"construction_year\"] = valid_mean\n\n    # in some columns 0 is an invalid value\n    x = replace_with_grouped_mean(df=x, value=0, column='longitude', to_groupby='region')\n    x = replace_with_grouped_mean(df=x, value=0, column='population', to_groupby='region')\n\n    # set latitude to the proper value\n    x = replace_with_grouped_mean(df=x, value=-2e-8, column='latitude', to_groupby='region')\n\n    # set amount_to non-zeroes\n    x = replace_with_grouped_mean(df=x, value=0, column='amount_tsh', to_groupby='region')\n\n    # remove na's\n    x = data_frame_imputer(df=x)\n\n    # print nans in the dataframe if any\n    print_nans(x)\n\n    # log transform numerical columns\n    num_cols = ['amount_tsh', 'population']\n    x[num_cols] = log_transformer(df=x[num_cols], base='e', c=1)\n    \n    print(\"data shape: \", x.shape)\n    print(\"Running label and one-hot encoding on the new data...\")\n    x = le.transform(x)\n    x = ohc.transform(x)\n    print(\"Processed data shape: \", x.shape)\n    print(\"done.\")\n    \n    return x"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["# load pickled models & transformers\nrf = joblib.load(Config.MODELS_DIR+'rf.pkl')\nle = joblib.load(Config.MODELS_DIR+'le.pkl')\nohc = joblib.load(Config.MODELS_DIR+'ohc.pkl')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["# get the data ready for prediction\ndf = pd.read_csv(Config.DATA_DIR+'new_pumps_data.csv', index_col=0)\ndf = process_data(df, le, ohc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\nChecking for NANs:............................\nYour selected dataframe has 12 columns and 23 rows \nThere are 0 columns that have missing values.\n..............................................\ndata shape:  (23, 12)\nRunning label and one-hot encoding on the new data...\nProcessed data shape:  (23, 39)\ndone.\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# make prediction\npredictions = rf.predict(df)\nprint(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[1 3 3 2 1 1 3 3 1 3 1 1 3 1 1 1 3 1 1 1 3 1 1]\n</div>"]}}],"execution_count":33}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.15","nbconvert_exporter":"python","file_extension":".py"},"name":"pumps-local","notebookId":159983768178610},"nbformat":4,"nbformat_minor":0}
