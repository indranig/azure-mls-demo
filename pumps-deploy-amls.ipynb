{"cells":[{"cell_type":"markdown","source":["<center><h1>PART 3: DEPLOY A MODEL ON AZURE MACHINE LEARNING SERVICE</h1></center>\n<br>\nIn this notebook, we will deploy the model we trained in Part 1 to Azure Machine Learning Service\n\n#### ABOUT THE MODEL & DATA\n\nUsing data from Taarifa and the Tanzanian Ministry of Water, we will predict which pumps are functional, which need some repairs, and which don't work at all. The labels encompass three classes and the training data is based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, portable water is available to communities across Tanzania. This competition is hosted on [Driven Data.](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/)\n\n#### PYTHON DEPENDENCIES\n```\nThis notebook was developed with the following packages:\nazureml-sdk[databricks]\ncategory-encoders==1.3.0\nnumpy==1.15.0\npandas==0.24.1\nscikit-learn==0.20.2\n\n```\n\n#### APPROACH\n\nAzure Kubernetes Service offers orchestrated elastic container clusters, that encapsulate the scoring logic and the model itself. The steps involved are:\n1. Configure the development environment  \n2. Test the model locally\n3. Create an execution script called `score.py`\n4. Configure the cluster image\n5. Deploy the cluster\n6. Test the web service\n\nReference: [Deploy models with the Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where)"],"metadata":{}},{"cell_type":"code","source":["from __future__ import print_function \nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.compute import AksCompute, ComputeTarget\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.model import Model\nfrom azureml.core.webservice import Webservice, AksWebservice\nfrom azureml.train.estimator import Estimator\n\n# set Pandas display options\npd.options.display.max_columns = None\n\n# check Azure SDK version\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/pythonVirtualEnvDirs/virtualEnv-1a8e2836-b680-490c-b06a-9f8983990e14/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\nAzure ML SDK Version:  1.0.15\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["#### 3.1 CONFIGURE THE DEVELOPMENT ENVIRONMENT\nConfiguring the development environment involves steps such as setting up an Azure ML Workspace, connecting to a remote storage and compute resources etc. \n\nFor local disk storage while development we will use the Databricks FileStore folder. `/FileStore` is a special folder within DBFS where you can save files and also download files to your local machine via a browser.  \nUse the Databricks Data menu/UI to upload the pumps_data.csv and new_pumps_data.csv to ``/FileStore/tables/pumps` directory (skip if you've already uploaded the data in Part 1 or 2).\n```\n/FileStore\n  ├── tables                     -> Databricks by default stores data here\n  │   └──pumps                   -> we will create this project specific folder\n  │      ├── new_pumps_data.csv  -> scoring dataset with no labels\n  │      └── pumps_data.csv      -> training dataset with labels\n  └── users/jason/pumps          -> we will create this folder as our project root folder\n      ├───models                 -> store all pickle files\n      │    ├──  local            -> pickle files created by training locally in the notebook\n      │    ├──  rf.pkl           -> Random Forest estimator trained on AMLS\n      │    ├──  le.pkl           -> Preprocessing transformer trained on AMLS\n      │    ├──  ohc.pkl          -> Preprocessing transformer trained on AMLS\n      │    ├──  y_le.pkl         -> Preprocessing transformer trained on AMLS\n      └── scripts                -> scripts such as train.py and score.py for AMLS\n```\n\nLet's create a `Config` class to hold all the pertinent configurations and storage locations."],"metadata":{}},{"cell_type":"code","source":["class Config(object):\n\n    # define Azure ML Workspace configuration variables---------\n    SUBSCRIPTION_ID = 'your_key_here'\n    RESOURCE_GROUP = 'ML_SANDBOX'\n    WORKSPACE_NAME = 'ML_SANDBOX'\n    WORKSPACE_REGION = 'East US 2'\n    \n    # setup Azure Machine Learning Service compute for training\n    TRAIN_COMPUTE = 'dev-vm'\n    \n    # Kubernetes cluster for deployment\n    DEPLOY_COMPUTE = 'dev-cluster'\n    IMAGE_NAME = 'pumps_rf_image'\n    AKS_SERVICE_NAME = 'pumps-aks-service-1'\n    \n    # define DBFS paths for sub-directories---------------------\n    # dbutils requires filepaths without the use of '/dbfs' so we will use this\n    # variables largely with dbutils functions.\n    PROJECT_DIR = '/FileStore/users/jason/pumps' \n    \n    # for Python to understand filepaths, you need to prefix '/dbfs'\n    MODELS_DIR = '/dbfs'+PROJECT_DIR+'models'\n    SCRIPTS_DIR = '/dbfs'+PROJECT_DIR+'scripts'\n    \n    # set location for uploading data\n    # default location for data is /FileStore/tables but we will use a pumps sub-directory\n    DATA_DIR = '/dbfs/FileStore/tables/pumps'  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["If you previously ran this code from the Part1 notebook, you can skip this."],"metadata":{}},{"cell_type":"code","source":["# connect to workspace\ntry:\n    ws = Workspace(subscription_id = Config.SUBSCRIPTION_ID, resource_group = Config.RESOURCE_GROUP, workspace_name = Config.WORKSPACE_NAME)\n    # write the details of the workspace to a configuration file to the notebook library\n    ws.write_config()\n    print(\"Workspace configuration succeeded.\")\nexcept:\n    print(\"Workspace not accessible.\")\n    \n# set up experiment\nexperiment_name = 'pumps-exp1'\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Falling back to use azure cli credentials. This fall back to use azure cli credentials will be removed in the next release. \nMake sure your code doesn&apos;t require &apos;az login&apos; to have happened before using azureml-sdk, except the case when you are specifying AzureCliAuthentication in azureml-sdk.\nPerforming interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code APQDUGZ6J to authenticate.\nInteractive authentication successfully completed.\nWrote the config file config.json to: /databricks/driver/aml_config/config.json\nWorkspace configuration succeeded.\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["#### 3.2 TEST THE MODEL LOCALLY\nBefore deploying the model on a remote cluster, let's test it locally to ensure it is working correctly"],"metadata":{}},{"cell_type":"code","source":["# NOT CURRENTLY WORKING----------------\n# download the registered pumps_rf model to the model folder in ADSL\nrf = Model(ws, 'pumps_rf')\nle = Model(ws, 'pumps_le')\nohc = Model(ws, 'pumps_ohc')\n\nrf.download(target_dir=Config.MODELS_DIR, exist_ok=True)\nle.download(target_dir=Config.MODELS_DIR, exist_ok=True)\nohc.download(target_dir=Config.MODELS_DIR, exist_ok=True)\n\n# verify the downloaded model file\ndbutils.fs.ls(Config.MODELS_DIR)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-110130823791346&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      8</span> ohc <span class=\"ansiyellow\">=</span> Model<span class=\"ansiyellow\">(</span>ws<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;pumps_ohc&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      9</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 10</span><span class=\"ansiyellow\"> </span>rf<span class=\"ansiyellow\">.</span>download<span class=\"ansiyellow\">(</span>target_dir<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;dbfs&apos;</span><span class=\"ansiyellow\">+</span>Config<span class=\"ansiyellow\">.</span>MODELS_DIR<span class=\"ansiyellow\">,</span> exist_ok<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">True</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     11</span> le<span class=\"ansiyellow\">.</span>download<span class=\"ansiyellow\">(</span>target_dir<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;dbfs&apos;</span><span class=\"ansiyellow\">+</span>Config<span class=\"ansiyellow\">.</span>MODELS_DIR<span class=\"ansiyellow\">,</span> exist_ok<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">True</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span> ohc<span class=\"ansiyellow\">.</span>download<span class=\"ansiyellow\">(</span>target_dir<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;dbfs&apos;</span><span class=\"ansiyellow\">+</span>Config<span class=\"ansiyellow\">.</span>MODELS_DIR<span class=\"ansiyellow\">,</span> exist_ok<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">True</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/azureml/core/model.py</span> in <span class=\"ansicyan\">download</span><span class=\"ansiblue\">(self, target_dir, exist_ok, exists_ok)</span>\n<span class=\"ansigreen\">    531</span>             raise AzureMLException(\n<span class=\"ansigreen\">    532</span>                 &quot;Illegal state. Unpack={}, Paths in target_dir is {}&quot;.format(self.unpack, file_paths))\n<span class=\"ansigreen\">--&gt; 533</span><span class=\"ansiyellow\">         </span>model_path <span class=\"ansiyellow\">=</span> os<span class=\"ansiyellow\">.</span>path<span class=\"ansiyellow\">.</span>commonpath<span class=\"ansiyellow\">(</span>file_paths<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    534</span>         <span class=\"ansigreen\">return</span> model_path<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    535</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/local_disk0/pythonVirtualEnvDirs/virtualEnv-391cc3f2-0943-4252-941f-1d5149ba5d72/lib/python3.5/posixpath.py</span> in <span class=\"ansicyan\">commonpath</span><span class=\"ansiblue\">(paths)</span>\n<span class=\"ansigreen\">    472</span>         <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;commonpath() arg is an empty sequence&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    473</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 474</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>paths<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> bytes<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    475</span>         sep <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">b&apos;/&apos;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    476</span>         curdir <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">b&apos;.&apos;</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: &apos;odict_values&apos; object does not support indexing</div>"]}}],"execution_count":8},{"cell_type":"code","source":["dbutils.fs.ls(Config.PROJECT_DIR+'/models')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>\n[FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/le.pkl&apos;, name=&apos;le.pkl&apos;, size=660),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/local/&apos;, name=&apos;local/&apos;, size=0),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/ohc.pkl&apos;, name=&apos;ohc.pkl&apos;, size=1237),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/rf.pkl&apos;, name=&apos;rf.pkl&apos;, size=194683395),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/y_le.pkl&apos;, name=&apos;y_le.pkl&apos;, size=359)]\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Define some helper functions"],"metadata":{}},{"cell_type":"code","source":["def print_nans(df):\n\n    print('Checking for NANs:............................')\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns={0: 'Missing Values', 1: '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n    print(\"Your selected dataframe has \" +\n          str(df.shape[1]) +\n          \" columns and \" +\n          str(len(df)) +\n          \" rows \\n\" \"There are \" +\n          str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n    print('..............................................')\n    return mis_val_table_ren_columns\n\n\ndef data_frame_imputer(df):\n    fill = pd.Series([df[c].value_counts().index[0]\n                      if df[c].dtype == np.dtype('O') else df[c].mean() for c in df],\n                     index=df.columns)\n    return df.fillna(fill)\n\n\ndef replace_with_grouped_mean(df, value, column, to_groupby):\n\n    invalid_mask = (df[column] == value)\n\n    # get the mean without the invalid value\n    means_by_group = (df[~invalid_mask].groupby(to_groupby)[column].mean())\n\n    # get an array of the means for all of the data\n    means_array = means_by_group[df[to_groupby].values].values\n\n    # assign the invalid values to means\n    df.loc[invalid_mask, column] = means_array[invalid_mask]\n\n    return df\n\n\ndef log_transformer(df, base, c=1):\n\n    if base == 'e' or base == np.e:\n        log = np.log\n\n    elif base == '10' or base == 10:\n        log = np.log10\n\n    else:\n        def log(x): return np.log(x) / np.log(base)\n\n    c = c\n    out = pd.DataFrame()\n    for _ in df:\n        out = df.apply(lambda x: log(x + c))\n    return out"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["The `process_data()` function will clean and preprocess any new data and output a format that the random forest model expects to see"],"metadata":{}},{"cell_type":"code","source":["def process_data(x, le, ohc):\n    \"\"\"\n    Gets new data ready for scoring\n\n    :param x: new data in the form of a dataframe\n    :param le: the pumps pickled label encoder transformer\n    :param ohc: the pumps pickled one-hot encoding transformer\n    :return:  dataframe ready for prediction\n    \"\"\"\n  \n    useful_columns = ['amount_tsh',\n                      'gps_height',\n                      'longitude',\n                      'latitude',\n                      'region',\n                      'population',\n                      'construction_year',\n                      'extraction_type_class',\n                      'management_group',\n                      'quality_group',\n                      'source_type',\n                      'waterpoint_type']\n\n    # subset to columns we care about\n    x = x[useful_columns]\n\n    # for column construction_year, values <=1000 are probably bad\n    invalid_rows = x['construction_year'] < 1000\n    valid_mean = int(x.construction_year[~invalid_rows].mean())\n    x.loc[invalid_rows, \"construction_year\"] = valid_mean\n\n    # in some columns 0 is an invalid value\n    x = replace_with_grouped_mean(df=x, value=0, column='longitude', to_groupby='region')\n    x = replace_with_grouped_mean(df=x, value=0, column='population', to_groupby='region')\n\n    # set latitude to the proper value\n    x = replace_with_grouped_mean(df=x, value=-2e-8, column='latitude', to_groupby='region')\n\n    # set amount_to non-zeroes\n    x = replace_with_grouped_mean(df=x, value=0, column='amount_tsh', to_groupby='region')\n\n    # remove na's\n    x = data_frame_imputer(df=x)\n\n    # print nans in the dataframe if any\n    print_nans(x)\n\n    # log transform numerical columns\n    num_cols = ['amount_tsh', 'population']\n    x[num_cols] = log_transformer(df=x[num_cols], base='e', c=1)\n    \n    print(\"data shape: \", x.shape)\n    print(\"Running label and one-hot encoding on the new data...\")\n    x = le.transform(x)\n    x = ohc.transform(x)\n    print(\"Processed data shape: \", x.shape)\n    print(\"done.\")\n    \n    return x"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# load pickled models & transformers\nrf = joblib.load(Config.MODELS_DIR+'/rf.pkl')\nle = joblib.load(Config.MODELS_DIR+'/le.pkl')\nohc = joblib.load(Config.MODELS_DIR+'/ohc.pkl')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# get the data ready for prediction\ndf = pd.read_csv(Config.DATA_DIR+'/new_pumps_data.csv', index_col=0)\ndf = process_data(df, le, ohc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\nChecking for NANs:............................\nYour selected dataframe has 12 columns and 23 rows \nThere are 0 columns that have missing values.\n..............................................\ndata shape:  (23, 12)\nRunning label and one-hot encoding on the new data...\nProcessed data shape:  (23, 39)\ndone.\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# make prediction\npredictions = rf.predict(df)\nprint(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[1 3 2 2 1 1 3 1 1 3 1 1 3 1 1 3 3 1 1 1 3 1 1]\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["#### 3.3 CREATE AN EXECUTION SCRIPT\nThe execution script receives data submitted to a deployed image, and passes it to the model. It then takes the response returned by the model and returns that to the client. The script is specific to your model; it must understand the data that the model expects and returns. The script usually contains two functions that load and run the model:\n\n`init()`: Typically this function loads the model into a global object. This function is run only once when the Docker container is started.\n\n`run(input_data)`: This function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization. You can also work with raw binary data. You can transform the data before sending to the model, or before returning to the client.\n\nTo encode & decode JSON, we will use the Pandas JSON functions with the `table` format. This format only only encodes the data, but the columns, indexes and schema. This makes the JSON explicit and clear."],"metadata":{}},{"cell_type":"code","source":["%%writefile /dbfs/mnt/jason/pumps/scripts/score.py\n\nimport json\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\nfrom azureml.core.model import Model\n\n###########################################################################################\n# HELPER FUNCTIONS\n###########################################################################################\n\ndef print_nans(df):\n\n    print('Checking for NANs:............................')\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns={0: 'Missing Values', 1: '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n    print(\"Your selected dataframe has \" +\n          str(df.shape[1]) +\n          \" columns and \" +\n          str(len(df)) +\n          \" rows \\n\" \"There are \" +\n          str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n    print('..............................................')\n    return mis_val_table_ren_columns\n\n\ndef data_frame_imputer(df):\n    fill = pd.Series([df[c].value_counts().index[0]\n                      if df[c].dtype == np.dtype('O') else df[c].mean() for c in df],\n                     index=df.columns)\n    return df.fillna(fill)\n\n\ndef replace_with_grouped_mean(df, value, column, to_groupby):\n\n    invalid_mask = (df[column] == value)\n\n    # get the mean without the invalid value\n    means_by_group = (df[~invalid_mask].groupby(to_groupby)[column].mean())\n\n    # get an array of the means for all of the data\n    means_array = means_by_group[df[to_groupby].values].values\n\n    # assign the invalid values to means\n    df.loc[invalid_mask, column] = means_array[invalid_mask]\n\n    return df\n\n\ndef log_transformer(df, base, c=1):\n\n    if base == 'e' or base == np.e:\n        log = np.log\n\n    elif base == '10' or base == 10:\n        log = np.log10\n\n    else:\n        def log(x): return np.log(x) / np.log(base)\n\n    c = c\n    out = pd.DataFrame()\n    for _ in df:\n        out = df.apply(lambda x: log(x + c))\n    return out\n\n\ndef process_data(x, le, ohc):\n    \"\"\"\n    Gets new data ready for scoring\n\n    :param x: new data in the form of a dataframe\n    :param le: the pumps pickled label encoder transformer\n    :param ohc: the pumps pickled one-hot encoding transformer\n    :return:  dataframe ready for prediction\n    \"\"\"\n  \n    useful_columns = ['amount_tsh',\n                      'gps_height',\n                      'longitude',\n                      'latitude',\n                      'region',\n                      'population',\n                      'construction_year',\n                      'extraction_type_class',\n                      'management_group',\n                      'quality_group',\n                      'source_type',\n                      'waterpoint_type']\n\n    # subset to columns we care about\n    x = x[useful_columns]\n\n    # for column construction_year, values <=1000 are probably bad\n    invalid_rows = x['construction_year'] < 1000\n    valid_mean = int(x.construction_year[~invalid_rows].mean())\n    x.loc[invalid_rows, \"construction_year\"] = valid_mean\n\n    # in some columns 0 is an invalid value\n    x = replace_with_grouped_mean(df=x, value=0, column='longitude', to_groupby='region')\n    x = replace_with_grouped_mean(df=x, value=0, column='population', to_groupby='region')\n\n    # set latitude to the proper value\n    x = replace_with_grouped_mean(df=x, value=-2e-8, column='latitude', to_groupby='region')\n\n    # set amount_to non-zeroes\n    x = replace_with_grouped_mean(df=x, value=0, column='amount_tsh', to_groupby='region')\n\n    # remove na's\n    x = data_frame_imputer(df=x)\n\n    # print nans in the dataframe if any\n    print_nans(x)\n\n    # log transform numerical columns\n    num_cols = ['amount_tsh', 'population']\n    x[num_cols] = log_transformer(df=x[num_cols], base='e', c=1)\n    \n    print(\"data shape: \", x.shape)\n    print(\"Running label and one-hot encoding on the new data...\")\n    x = le.transform(x)\n    x = ohc.transform(x)\n    print(\"Processed data shape: \", x.shape)\n    print(\"done.\")\n    \n    return x\n\n###########################################################################################\n# MAIN\n###########################################################################################\n\n# load the model\ndef init():\n    \"\"\"\n    Loads the models and estimators into a global scope\n    \"\"\"\n    \n    global rf\n    global le\n    global ohc\n\n    # retrieve model\n    rf_path = Model.get_model_path('pumps_rf')\n    rf = joblib.load(model_path)\n    \n    # retrieve transformers\n    le_path = Model.get_model_path('pumps_le')\n    ohc_path = Model.get_model_path('pumps_ohc')\n    le = joblib.load(le_path)\n    ohc = joblib.load(ohc_path)\n\n# Passes data to the model and returns the prediction\ndef run(raw_data):\n    \"\"\"\n    Processes the incoming data, passes it to the\n    model and outputs predictions in JSON format\n    \"\"\"\n    json_data = json.loads(raw_data)\n    df = pd.read_json(json_data, orient='table')\n    \n    # process data\n    processed_data = process_data(df, le, ohc):\n    \n    # make prediction\n    preds = rf.predict(processed_data)\n    \n    # join predictions to index and jsonify\n    # TODO\n    \n    \n    return json.dumps(y_hat.tolist())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["#### 3.4 CONFIGURE AN AKS CLUSTER\nDeployed models are packaged as an image. The image contains the dependencies needed to run the model. Will need to create an environment file (`myenv.yml`) that specifies all of the scoring script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image by Azure ML."],"metadata":{}},{"cell_type":"code","source":["package_list = [\n  'category-encoders==1.3.0',\n  'numpy==1.15.0',\n  'pandas==0.24.1',\n  'scikit-learn==0.20.2']\n\n# Conda environment configuration\nmyenv = CondaDependencies.create(pip_packages=package_list)\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n    \nprint(myenv.serialize_to_string())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["For Azure Container Instance, Azure Kubernetes Service, and Azure IoT Edge deployments, the `azureml.core.image.ContainerImage` class is used to create an image configuration. The image configuration is then used to create a new Docker image."],"metadata":{}},{"cell_type":"code","source":["# retrieve cloud representations of the models\nrf = Model(workspace=ws, name='pumps_rf')\nle = Model(workspace=ws, name='pumps_le')\nohc = Model(workspace=ws, name='pumps_ohc')\nprint(rf); print(le); print(ohc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;azureml.core.model.Model object at 0x7f92480187f0&gt;\n&lt;azureml.core.model.Model object at 0x7f9248018d68&gt;\n&lt;azureml.core.model.Model object at 0x7f92484cb550&gt;\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# Image configuration\nimage_config = ContainerImage.image_configuration(execution_script='score.py', \n                                                  runtime='python', \n                                                  conda_file='myenv.yml',\n                                                  description='Pumps Random Forest model)\n\n\n# Register the image from the image configuration\nimage = ContainerImage.create(name = Config.IMAGE_NAME, \n                              models = [rf, le, ohc],\n                              image_config = image_config,\n                              workspace = ws)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["#### 3.5 DEPLOY THE AKS CLUSTER\nTo begin with, let's attach an existing AKS cluster to the AML workspace"],"metadata":{}},{"cell_type":"code","source":["# Attach the cluster to your workgroup\nattach_config = AksCompute.attach_configuration(resource_group = Config.RESOURCE_GROUP,\n                                                cluster_name = Config.DEPLOY_COMPUTE)\naks_target = ComputeTarget.attach(workspace=ws, \n                                  name=Config.DEPLOY_COMPUTE, \n                                  attach_configuration=attach_config)\n\n# Wait for the operation to complete\naks_target.wait_for_completion(True)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Next, let's deploy the cluster with the image we configured in the previous section."],"metadata":{}},{"cell_type":"code","source":["# Set configuration and service name\naks_config = AksWebservice.deploy_configuration()\n\n# Deploy from image\nservice = Webservice.deploy_from_image(workspace = ws,\n                                       name = Config.AKS_SERVICE_NAME,\n                                       image = image,\n                                       deployment_config = aks_config,\n                                       deployment_target = aks_target)\n# Wait for the deployment to complete\nservice.wait_for_deployment(show_output = True)\nprint(service.state)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# in case of issues, you can check the logs\nservice.get_logs()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# This is the HTTP endpoint that accepts REST client calls\nprint(service.scoring_uri)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### 3.6 TEST THE WEB SERVICE\nLet's send the data as a JSON string to the web service hosted in AKS and use the SDK's run API to invoke the service. Here we will take an image from our validation data to predict on."],"metadata":{}},{"cell_type":"code","source":["import torch\nfrom torchvision import transforms\n    \ndef preprocess(image_file):\n    \"\"\"Preprocess the input image.\"\"\"\n    data_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    image = Image.open(image_file)\n    image = data_transforms(image).float()\n    image = torch.tensor(image)\n    image = image.unsqueeze(0)\n    return image.numpy()\n  \ninput_data = preprocess('test_img.jpg')\nresult = service.run(input_data=json.dumps({'data': input_data.tolist()}))\nprint(result)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#### TEST JSON"],"metadata":{}},{"cell_type":"code","source":["test_data ='''\n{\n    \"schema\": {\n        \"pandas_version\": \"0.20.0\",\n        \"primaryKey\": [\"id\"],\n        \"fields\": [\n            {\"name\": \"id\",\"type\": \"integer\"},\n            {\"name\": \"amount_tsh\",\"type\": \"integer\"},\n            {\"name\": \"date_recorded\",\"type\": \"string\"},\n            {\"name\": \"funder\",\"type\": \"string\"},\n            {\"name\": \"gps_height\",\"type\": \"integer\"},\n            {\"name\": \"installer\",\"type\": \"string\"},\n            {\"name\": \"longitude\",\"type\": \"number\"},\n            {\"name\": \"latitude\",\"type\": \"number\"},\n            {\"name\": \"wpt_name\",\"type\": \"string\"},\n            {\"name\": \"num_private\",\"type\": \"integer\"},\n            {\"name\": \"basin\",\"type\": \"string\"},\n            {\"name\": \"subvillage\",\"type\": \"string\"},\n            {\"name\": \"region\",\"type\": \"string\"},\n            {\"name\": \"region_code\",\"type\": \"integer\"},\n            {\"name\": \"district_code\",\"type\": \"integer\"},\n            {\"name\": \"lga\",\"type\": \"string\"},\n            {\"name\": \"ward\",\"type\": \"string\"},\n            {\"name\": \"population\",\"type\": \"integer\"},\n            {\"name\": \"public_meeting\",\"type\": \"string\"},\n            {\"name\": \"recorded_by\",\"type\": \"string\"},\n            {\"name\": \"scheme_management\",\"type\": \"string\"},\n            {\"name\": \"scheme_name\",\"type\": \"string\"},\n            {\"name\": \"permit\",\"type\": \"string\"},\n            {\"name\": \"construction_year\",\"type\": \"integer\"},\n            {\"name\": \"extraction_type\",\"type\": \"string\"},\n            {\"name\": \"extraction_type_group\",\"type\": \"string\"},\n            {\"name\": \"extraction_type_class\",\"type\": \"string\"},\n            {\"name\": \"management\",\"type\": \"string\"},\n            {\"name\": \"management_group\",\"type\": \"string\"},\n            {\"name\": \"payment\",\"type\": \"string\"},\n            {\"name\": \"payment_type\",\"type\": \"string\"},\n            {\"name\": \"water_quality\",\"type\": \"string\"},\n            {\"name\": \"quality_group\",\"type\": \"string\"},\n            {\"name\": \"quantity\",\"type\": \"string\"},\n            {\"name\": \"quantity_group\",\"type\": \"string\"},\n            {\"name\": \"source\",\"type\": \"string\"},\n            {\"name\": \"source_type\",\"type\": \"string\"},\n            {\"name\": \"source_class\",\"type\": \"string\"},\n            {\"name\": \"waterpoint_type\",\"type\": \"string\"},\n            {\"name\": \"waterpoint_type_group\",\"type\": \"string\"}\n        ]\n    },\n    \"data\": [\n        {\n            \"id\": 61848,\n            \"amount_tsh\": 0,\n            \"date_recorded\": \"2011-08-04\",\n            \"funder\": \"Rudep\",\n            \"gps_height\": 1645,\n            \"installer\": \"DWE\",\n            \"longitude\": 31.44412134,\n            \"latitude\": -8.27496163,\n            \"wpt_name\": \"Kwa Juvenal Ching'Ombe\",\n            \"num_private\": 0,\n            \"basin\": \"Lake Tanganyika\",\n            \"subvillage\": \"Tunzi\",\n            \"region\": \"Rukwa\",\n            \"region_code\": 15,\n            \"district_code\": 2,\n            \"lga\": \"Sumbawanga Rural\",\n            \"ward\": \"Mkowe\",\n            \"population\": 200,\n            \"public_meeting\": true,\n            \"recorded_by\": \"GeoData Consultants Ltd\",\n            \"scheme_management\": \"VWC\",\n            \"scheme_name\": null,\n            \"permit\": false,\n            \"construction_year\": 1991,\n            \"extraction_type\": \"swn 80\",\n            \"extraction_type_group\": \"swn 80\",\n            \"extraction_type_class\": \"handpump\",\n            \"management\": \"vwc\",\n            \"management_group\": \"user-group\",\n            \"payment\": \"never pay\",\n            \"payment_type\": \"never pay\",\n            \"water_quality\": \"soft\",\n            \"quality_group\": \"good\",\n            \"quantity\": \"enough\",\n            \"quantity_group\": \"enough\",\n            \"source\": \"machine dbh\",\n            \"source_type\": \"borehole\",\n            \"source_class\": \"groundwater\",\n            \"waterpoint_type\": \"hand pump\",\n            \"waterpoint_type_group\": \"hand pump\"\n        },\n        {\n            \"id\": 48451,\n            \"amount_tsh\": 500,\n            \"date_recorded\": \"2011-07-04\",\n            \"funder\": \"Unicef\",\n            \"gps_height\": 1703,\n            \"installer\": \"DWE\",\n            \"longitude\": 34.64243884,\n            \"latitude\": -9.10618458,\n            \"wpt_name\": \"Kwa John Mtenzi\",\n            \"num_private\": 0,\n            \"basin\": \"Rufiji\",\n            \"subvillage\": \"Kidudumo\",\n            \"region\": \"Iringa\",\n            \"region_code\": 11,\n            \"district_code\": 4,\n            \"lga\": \"Njombe\",\n            \"ward\": \"Mdandu\",\n            \"population\": 35,\n            \"public_meeting\": true,\n            \"recorded_by\": \"GeoData Consultants Ltd\",\n            \"scheme_management\": \"WUA\",\n            \"scheme_name\": \"wanging'ombe water supply s\",\n            \"permit\": true,\n            \"construction_year\": 1978,\n            \"extraction_type\": \"gravity\",\n            \"extraction_type_group\": \"gravity\",\n            \"extraction_type_class\": \"gravity\",\n            \"management\": \"wua\",\n            \"management_group\": \"user-group\",\n            \"payment\": \"pay monthly\",\n            \"payment_type\": \"monthly\",\n            \"water_quality\": \"soft\",\n            \"quality_group\": \"good\",\n            \"quantity\": \"dry\",\n            \"quantity_group\": \"dry\",\n            \"source\": \"river\",\n            \"source_type\": \"river\\/lake\",\n            \"source_class\": \"surface\",\n            \"waterpoint_type\": \"communal standpipe\",\n            \"waterpoint_type_group\": \"communal standpipe\"\n        }\n    ]\n}\n'''"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["new_df = pd.read_json(test_data, orient='table')\nnew_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">47</span><span class=\"ansired\">]: </span>\n       amount_tsh date_recorded  funder  gps_height installer  longitude  \\\nid                                                                         \n61848           0    2011-08-04   Rudep        1645       DWE  31.444121   \n48451         500    2011-07-04  Unicef        1703       DWE  34.642439   \n\n       latitude                wpt_name  num_private            basin  \\\nid                                                                      \n61848 -8.274962  Kwa Juvenal Ching&apos;Ombe            0  Lake Tanganyika   \n48451 -9.106185         Kwa John Mtenzi            0           Rufiji   \n\n      subvillage  region  region_code  district_code               lga  \\\nid                                                                       \n61848      Tunzi   Rukwa           15              2  Sumbawanga Rural   \n48451   Kidudumo  Iringa           11              4            Njombe   \n\n         ward  population  public_meeting              recorded_by  \\\nid                                                                   \n61848   Mkowe         200               1  GeoData Consultants Ltd   \n48451  Mdandu          35               1  GeoData Consultants Ltd   \n\n      scheme_management                  scheme_name  permit  \\\nid                                                             \n61848               VWC                         None       0   \n48451               WUA  wanging&apos;ombe water supply s       1   \n\n       construction_year extraction_type extraction_type_group  \\\nid                                                               \n61848               1991          swn 80                swn 80   \n48451               1978         gravity               gravity   \n\n      extraction_type_class management management_group      payment  \\\nid                                                                     \n61848              handpump        vwc       user-group    never pay   \n48451               gravity        wua       user-group  pay monthly   \n\n      payment_type water_quality quality_group quantity quantity_group  \\\nid                                                                       \n61848    never pay          soft          good   enough         enough   \n48451      monthly          soft          good      dry            dry   \n\n            source source_type source_class     waterpoint_type  \\\nid                                                                \n61848  machine dbh    borehole  groundwater           hand pump   \n48451        river  river/lake      surface  communal standpipe   \n\n      waterpoint_type_group  \nid                           \n61848             hand pump  \n48451    communal standpipe  \n</div>"]}}],"execution_count":34},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"name":"pumps-azuremls","notebookId":1326659822288856},"nbformat":4,"nbformat_minor":0}
