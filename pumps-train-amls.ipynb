{"cells":[{"cell_type":"markdown","source":["<center><h1>PART 2: TRAIN A MODEL ON AZURE MACHINE LEARNING SERVICE</h1></center>\n<br>\n\nIn this notebook, we will train a non-trivial machine learning model using Azure Machine Learning Service. In \n\n#### ABOUT THE MODEL & DATA\n\nUsing data from Taarifa and the Tanzanian Ministry of Water, we will predict which pumps are functional, which need some repairs, and which don't work at all. The labels encompass three classes and the training data is based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, portable water is available to communities across Tanzania. This competition is hosted on [Driven Data.](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/)\n\n#### PYTHON DEPENDENCIES\n```\nThis notebook was developed with the following packages:\nazureml-sdk[databricks]\ncategory-encoders==1.3.0\nnumpy==1.15.0\npandas==0.24.1\nscikit-learn==0.20.2\n\n```\n\n#### APPROACH\nWe will use a Machine Learning Compute resource on AMLS to train the model. This is simply a VM. Training a model on Azure Machine Learning Service involves the following steps:\n1. Configure the development environment  \n2. Upload your data to blob storage\n3. Create a training script called `train.py`\n4. Submit the job\n5. Register the final model\n\nResource: [How Azure Machine Learning service works: Architecture and concepts](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace)"],"metadata":{}},{"cell_type":"code","source":["from __future__ import print_function \nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.compute import AmlCompute, AksCompute, ComputeTarget\nfrom azureml.train.estimator import Estimator\nfrom azureml.train.hyperdrive import GridParameterSampling, BanditPolicy, choice\nfrom azureml.train.hyperdrive import HyperDriveRunConfig, PrimaryMetricGoal\n\n# set Pandas display options\npd.options.display.max_columns = None\n\n# check Azure SDK version\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Azure ML SDK Version:  1.0.15\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["#### 2.1 CONFIGURE THE DEVELOPMENT ENVIRONMENT\nConfiguring the development environment involves steps such as setting up an Azure ML Workspace, connecting to a remote storage and compute resources etc. \n\nFor local disk storage while development we will use the Databricks FileStore folder. `/FileStore` is a special folder within DBFS where you can save files and also download files to your local machine via a browser.  \nUse the Databricks Data menu/UI to upload the pumps_data.csv and new_pumps_data.csv to ``/FileStore/tables/pumps` directory (skip if you've already uploaded the data in Part 1).\n```\n/FileStore\n  ├── tables                     -> Databricks by default stores data here\n  │   └──pumps                   -> we will create this project specific folder\n  │      ├── new_pumps_data.csv  -> scoring dataset with no labels\n  │      └── pumps_data.csv      -> training dataset with labels\n  └── users/jason/pumps          -> we will create this folder as our project root folder\n      ├───models                 -> store all pickle files\n      │    ├──  local            -> pickle files created by training locally in the notebook\n      │    ├──  rf.pkl           -> Random Forest estimator trained on AMLS\n      │    ├──  le.pkl           -> Preprocessing transformer trained on AMLS\n      │    ├──  ohc.pkl          -> Preprocessing transformer trained on AMLS\n      │    ├──  y_le.pkl         -> Preprocessing transformer trained on AMLS\n      └── scripts                -> scripts such as train.py and score.py for AMLS\n```\n\nLet's create a `Config` class to hold all the pertinent configurations and storage locations. \n\nResource: [Configure a development environment for Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment)"],"metadata":{}},{"cell_type":"code","source":["class Config(object):\n\n    # define Azure ML Workspace configuration variables---------\n    SUBSCRIPTION_ID = 'your_key_here'\n    RESOURCE_GROUP = 'ML_SANDBOX'\n    WORKSPACE_NAME = 'ML_SANDBOX'\n    WORKSPACE_REGION = 'East US 2'\n    \n    # setup Azure Machine Learning Service compute for training\n    TRAIN_COMPUTE = 'dev-vm'\n    \n    # Kubernetes cluster for deployment\n    DEPLOY_COMPUTE = 'dev-cluster'\n    IMAGE_NAME = 'pumps_rf_image'\n    AKS_SERVICE_NAME = 'pumps-aks-service-1'\n    \n    # define DBFS paths for sub-directories---------------------\n    # dbutils requires filepaths without the use of '/dbfs' so we will use this\n    # variables largely with dbutils functions.\n    PROJECT_DIR = '/FileStore/users/jason/pumps' \n    \n    # for Python to understand filepaths, you need to prefix '/dbfs'\n    MODELS_DIR = '/dbfs'+PROJECT_DIR+'/models'\n    SCRIPTS_DIR = '/dbfs'+PROJECT_DIR+'/scripts'\n    \n    # set location for uploading data\n    # default location for data is /FileStore/tables but we will use a pumps sub-directory\n    DATA_DIR = '/dbfs/FileStore/tables/pumps'  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Once the directories are created, use the Databricks Data menu/UI to upload the `pumps_data.csv` and `new_pumps_data.csv` to `/FileStore/tables/pumps` directory"],"metadata":{}},{"cell_type":"code","source":["# create the project directories in FileStore if not already exists\ndbutils.fs.mkdirs(Config.PROJECT_DIR+'/models')\ndbutils.fs.mkdirs(Config.PROJECT_DIR+'/scripts')\n# verify\ndbutils.fs.ls(Config.PROJECT_DIR)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>\n[FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/models/&apos;, name=&apos;models/&apos;, size=0),\n FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/scripts/&apos;, name=&apos;scripts/&apos;, size=0)]\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["The Azure Machine Learning Workspace provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning service."],"metadata":{}},{"cell_type":"code","source":["# connect to workspace\ntry:\n    ws = Workspace(subscription_id = Config.SUBSCRIPTION_ID, resource_group = Config.RESOURCE_GROUP, workspace_name = Config.WORKSPACE_NAME)\n    # write the details of the workspace to a configuration file to the notebook library\n    ws.write_config()\n    print(\"Workspace configuration succeeded.\")\nexcept:\n    print(\"Workspace not accessible.\")\n    \n# set up experiment\nexperiment_name = 'pumps-exp1'\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote the config file config.json to: /databricks/driver/aml_config/config.json\nWorkspace configuration succeeded.\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# attach compute cluster\nif Config.TRAIN_COMPUTE in ws.compute_targets:\n    compute_target = ws.compute_targets[Config.TRAIN_COMPUTE]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('Compute Target Found: ' + Config.TRAIN_COMPUTE)\nelse:\n    print('No cluster found')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Compute Target Found: dev-vm\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["#### 2.2 UPLOAD DATA TO BLOB STORAGE\nData has to be uploaded to specific Blob storage buckets linked to Azure Machine Learning Service so that it is accessable to the remote compute clusters. The pumps files are uploaded into a directory named `pumps` at the root of the datastore."],"metadata":{}},{"cell_type":"code","source":["# upload data to Azure Machine Learning Service datastore\nds = ws.get_default_datastore()\nprint('Datastore Type : '+ds.datastore_type)\nprint('Account Name   : '+ds.account_name)\nprint('Container Name : '+ds.container_name)\n\nds.upload(src_dir = Config.DATA_DIR, \n          target_path = 'pumps', \n          overwrite = True, \n          show_progress = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Datastore Type : AzureBlob\nAccount Name   : mlsandbox0450541102\nContainer Name : azureml-blobstore-4ed29ebd-ada3-45c8-ad78-b45acf6c15aa\nUploading /dbfs/FileStore/tables/pumps/new_pumps_data.csv\nUploading /dbfs/FileStore/tables/pumps/pumps_data.csv\nUploading /dbfs/FileStore/tables/pumps/y_le.pkl\nUploaded /dbfs/FileStore/tables/pumps/y_le.pkl, 1 files out of an estimated total of 3\nUploaded /dbfs/FileStore/tables/pumps/new_pumps_data.csv, 2 files out of an estimated total of 3\nUploaded /dbfs/FileStore/tables/pumps/pumps_data.csv, 3 files out of an estimated total of 3\n<span class=\"ansired\">Out[</span><span class=\"ansired\">11</span><span class=\"ansired\">]: </span>$AZUREML_DATAREFERENCE_89b28ea9a6b34247aa61acab48b56b08\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["#### 2.3 CREATE A TRAINING SCRIPT\nTraining on a remote cluster involves create a `train.py` script along with pointers to the data location and any other objects such as estimators. When using `%%writefile` you need to prefix `/dbfs` to the filesystem path"],"metadata":{}},{"cell_type":"code","source":["%%writefile /dbfs/FileStore/users/jason/pumps/scripts/train.py\n\nimport os\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport category_encoders as ce\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\n\nfrom azureml.core import Run\n# get hold of the current run\nrun_logger = Run.get_context()\n\n###########################################################################################\n# HELPER FUNCTIONS\n###########################################################################################\n\ndef create_dataframe(x):\n    \"\"\"\n    Imports the pumps csv data file directly from AML blob storage.\n\n    :param x: full path to a csv file\n              e.g: '/dbfs/mnt/jason/pumps/data/pumps_data.csv'\n    :return: two dataframes that split data and labels\n    \"\"\"\n    # import raw data\n    raw = pd.read_csv(x, index_col=0)\n    labels = pd.DataFrame(raw['status_group'])\n    data = raw.drop('status_group', axis=1)\n    \n    return data, labels\n\ndef print_nans(df):\n\n    print('Checking for NANs:............................')\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns={0: 'Missing Values', 1: '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n    print(\"Your selected dataframe has \" +\n          str(df.shape[1]) +\n          \" columns and \" +\n          str(len(df)) +\n          \" rows \\n\" + \"There are \" +\n          str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n    print('..............................................')\n    return mis_val_table_ren_columns\n\n\ndef data_frame_imputer(df):\n    fill = pd.Series([df[c].value_counts().index[0]\n                      if df[c].dtype == np.dtype('O') else df[c].mean() for c in df],\n                     index=df.columns)\n    return df.fillna(fill)\n\n\ndef replace_with_grouped_mean(df, value, column, to_groupby):\n\n    invalid_mask = (df[column] == value)\n\n    # get the mean without the invalid value\n    means_by_group = (df[~invalid_mask].groupby(to_groupby)[column].mean())\n\n    # get an array of the means for all of the data\n    means_array = means_by_group[df[to_groupby].values].values\n\n    # assign the invalid values to means\n    df.loc[invalid_mask, column] = means_array[invalid_mask]\n\n    return df\n\n\ndef log_transformer(df, base, c=1):\n\n    if base == 'e' or base == np.e:\n        log = np.log\n\n    elif base == '10' or base == 10:\n        log = np.log10\n\n    else:\n        def log(x): return np.log(x) / np.log(base)\n\n    c = c\n    out = pd.DataFrame()\n    for _ in df:\n        out = df.apply(lambda x: log(x + c))\n    return out\n\n\ndef stratified_split(x, y, test_size):\n\n    from sklearn.model_selection import StratifiedShuffleSplit\n\n    sss = StratifiedShuffleSplit(n_splits=10, test_size=test_size, random_state=5)\n    sss.get_n_splits(x, y)\n    data_train = pd.DataFrame()\n    data_test = pd.DataFrame()\n    label_train = pd.DataFrame()\n    label_test = pd.DataFrame()\n    for train_index, test_index in sss.split(x, y):\n        data_train, data_test = x.iloc[train_index], x.iloc[test_index]\n        label_train, label_test = y.iloc[train_index], y.iloc[test_index]\n    return data_train, data_test, label_train, label_test\n  \n###########################################################################################\n# PREPROCESSING\n###########################################################################################\n  \ndef clean_data(x, y):\n    \"\"\"\n    Takes the pumps data and label dataframe and cleans it\n\n    :param x: the pumps dataframe\n    :param y: the pumps labels dataframe\n    :return:  stratified splits for train and test\n    \"\"\"\n  \n    useful_columns = ['amount_tsh',\n                      'gps_height',\n                      'longitude',\n                      'latitude',\n                      'region',\n                      'population',\n                      'construction_year',\n                      'extraction_type_class',\n                      'management_group',\n                      'quality_group',\n                      'source_type',\n                      'waterpoint_type']\n\n    # subset to columns we care about\n    x = x[useful_columns]\n\n    # for column construction_year, values <=1000 are probably bad\n    invalid_rows = x['construction_year'] < 1000\n    valid_mean = int(x.construction_year[~invalid_rows].mean())\n    x.loc[invalid_rows, \"construction_year\"] = valid_mean\n\n    # in some columns 0 is an invalid value\n    x = replace_with_grouped_mean(df=x, value=0, column='longitude', to_groupby='region')\n    x = replace_with_grouped_mean(df=x, value=0, column='population', to_groupby='region')\n\n    # set latitude to the proper value\n    x = replace_with_grouped_mean(df=x, value=-2e-8, column='latitude', to_groupby='region')\n\n    # set amount_to non-zeroes\n    x = replace_with_grouped_mean(df=x, value=0, column='amount_tsh', to_groupby='region')\n\n    # remove na's\n    x = data_frame_imputer(df=x)\n\n    # print nans in the dataframe if any\n    print_nans(x)\n\n    # log transform numerical columns\n    num_cols = ['amount_tsh', 'population']\n    x[num_cols] = log_transformer(df=x[num_cols], base='e', c=1)\n\n    # do train/test split\n    x_train, x_test, y_train, y_test = stratified_split(x=x, y=y, test_size=0.2)\n\n    return x_train, x_test, y_train, y_test\n  \n  \ndef train_pre_processing(x, y):\n\n    \"\"\"\n    Preprocesses the pumps train datasets by applying label\n    and one-hot encoding\n\n    :param x: the pumps x_train dataset\n    :param y: the upmps y_train dataset\n    :return: encoded datasets and the fitted transformers\n    \"\"\"\n\n    # transform categorical variables with encoders\n    le_cols = ['region']\n    ohc_cols = ['extraction_type_class',\n                'management_group',\n                'quality_group',\n                'source_type',\n                'waterpoint_type']\n\n    # define encoders include label encoding for the actual labels\n    # using handle_unknown='ignore' will leave out new unseen values so keep\n    # monitoring your data for changes\n    \n    le = ce.OrdinalEncoder(cols=le_cols,\n                           return_df=True,\n                           handle_unknown='ignore')\n\n    ohc = ce.OneHotEncoder(cols=ohc_cols,\n                           return_df=True,\n                           use_cat_names=False,\n                           handle_unknown='ignore')\n\n    y_le = ce.OrdinalEncoder(return_df=True,\n                             handle_unknown='ignore')\n\n    print(\"x_train shape: \", x.shape)\n    print(\"y_train shape: \", y.shape)\n    # apply the encoders\n    print(\"Running label and one-hot encoding on the train data...\")\n    x = le.fit_transform(x)\n    x = ohc.fit_transform(x)\n    y = y_le.fit_transform(y)\n    # update the transformers\n    le = le\n    ohc = ohc\n    y_le = y_le\n    print(\"Final x_train shape: \", x.shape)\n    print(\"Final y_train shape: \", y.shape)\n    print(\"done.\")\n    \n    return x, y, le, ohc, y_le\n\n\ndef test_pre_processing(x, y, le, ohc, y_le):\n\n    \"\"\"\n    Preprocesses the pumps test datasets by applying the fitted label\n    and one-hot encoding transformers to the test/validation datasets\n\n    :param x: the x_test dataset\n    :param y: the y_test dataset\n    :param le: the label encoder fitted from the train_pre_processing() function\n    :param ohc: the one-hot encoder fitted from the train_pre_processing() function\n    :param y_le: the y label encoder fitted from the train_pre_processing() function\n    :return: encoded x_test and y_test\n    \"\"\"\n  \n    print(\"x_test shape: \", x.shape)\n    print(\"y_test shape: \", y.shape)\n    print(\"Running label and one-hot encoding on the test data...\")\n    x = le.transform(x)\n    x = ohc.transform(x)\n    y = y_le.transform(y)\n    print(\"New x_test shape: \", x.shape)\n    print(\"New y_test shape: \", y.shape)\n    print(\"done.\")\n\n    return x, y\n  \n  \n###########################################################################################\n# TRAINING\n###########################################################################################\n\ndef train_and_evaluate(x, n_estimators=100, criterion='entropy', class_weight='balanced_subsample'):\n    \"\"\"\n    A full pipeline that cleans, preprocesses data and then fits a\n    random forest classifier\n\n    :param x              : full location of the Azure blob storage\n                            e.g: os.path.join(args.data_folder, 'pumps')\n    :param n_estimators:  : random forest parameter for number of branches\n    :param criterion:     : random forest parameter for node splitting methodology\n    :param class_weight:  : random forest parameter whether to treat all classes as balanced\n    :return               :  all estimator/transfromer objects and the accuracy metric\n    \"\"\"\n\n    # ingest and process data\n    data, labels = create_dataframe(x=x)\n    x_train, x_test, y_train, y_test = clean_data(x=data, y=labels)\n    x_train, y_train, le, ohc, y_le = train_pre_processing(x=x_train, y=y_train)\n    x_test, y_test = test_pre_processing(x=x_test, y=y_test, le=le, ohc=ohc, y_le=y_le)\n    \n    # train classifier\n    print(\"training classifier...\")\n    rf = RandomForestClassifier(n_estimators=n_estimators,\n                                criterion=criterion,\n                                class_weight=class_weight)\n    rf.fit(x_train, np.ravel(y_train))\n    print(\" classifier has been trained\")\n    \n    # evaluate on test set\n    test_pred = rf.predict(x_test)\n    accuracy = accuracy_score(y_test, test_pred)\n    print(\"Test Accuracy: \", accuracy)\n    \n    # we need to return the transformers also so that it gets captured in the global scope\n    # this will allow us to save these models as pickle files\n    return rf, accuracy, le, ohc, y_le\n  \n  \n###########################################################################################\n# MAIN\n###########################################################################################\n\ndef main(): \n  \n    # create four arguments to specify location of the data and the \n    # Random Forest hyperparameters \n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n      '--data-folder', \n      help='data folder mounting point',\n      type=str, \n      dest='data_folder'\n    )\n\n    parser.add_argument(\n      '--n_estimators', \n      help='The number of trees in the forest.',\n      type=int, \n      dest='n_estimators', \n      default=100\n    )\n\n    parser.add_argument(\n      '--criterion', \n      help='The function to measure the quality of a split. Supported criteria are “gini” \\\n      for the Gini impurity and “entropy” for the information gain. ',\n      type=str, \n      dest='criterion', \n      default='entropy'\n    )\n\n    parser.add_argument(\n      '--class_weight', \n      help='Specify class weights. Options are \"balanced\", \"balanced_subsample\" or \"None\".',\n      type=str, \n      dest='class_weight', \n      default='balanced_subsample'\n    )\n\n    args = parser.parse_args()\n\n    # specify the pumps folder within the AML blob storage as the folder with data\n    data_folder = os.path.join(args.data_folder, 'pumps')\n    print('Data folder:', data_folder)\n\n    # run the entire pipeline\n    rf, accuracy, le, ohc, y_le = train_and_evaluate(x = os.path.join(data_folder, 'pumps_data.csv'), \n                                                     n_estimators = args.n_estimators,\n                                                     criterion = args.criterion,\n                                                     class_weight = args.class_weight)\n    \n    # log accuracy\n    run_logger.log('accuracy', np.float(accuracy))\n\n    # save model objects\n    # note file saved in the outputs folder is automatically uploaded into experiment record\n    os.makedirs('outputs', exist_ok=True)\n    joblib.dump(value=rf, filename='outputs/rf.pkl')\n    joblib.dump(value=le, filename='outputs/le.pkl')\n    joblib.dump(value=ohc, filename='outputs/ohc.pkl')\n    joblib.dump(value=y_le, filename='outputs/y_le.pkl')\n\n    \nif __name__ == \"__main__\":\n    main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Overwriting /dbfs/FileStore/users/jason/pumps/scripts/train.py\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["dbutils.fs.ls(Config.PROJECT_DIR+'/scripts')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>[FileInfo(path=&apos;dbfs:/FileStore/users/jason/pumps/scripts/train.py&apos;, name=&apos;train.py&apos;, size=12265)]\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Files stored in /FileStore are accessible in your web browser. For example, the file you stored in `/FileStore/my-stuff/my-file.txt` is accessible at:  \n`https://<your-region>.azuredatabricks.net/files/my-stuff/my-file.txt?o=######`\n\nYou will find the value of `o` in the URL of your browser.\nTo verify the file, you can use the URL endpoint:  \n<https://eastus2.azuredatabricks.net/files/users/jason/pumps/scripts/train.py?o=3749457624382033>"],"metadata":{}},{"cell_type":"markdown","source":["#### 2.4 SUBMIT THE JOB\nAn estimator object needs to be created before submitting a job to the remote compute cluster. Will will also incorporate a grid search for the hyperparameters using `GridParameterSampling`. \n\nReference: [Train models with Azure Machine Learning using estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models)  \nReference: [Tune hyperparameters for your model with Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy)"],"metadata":{}},{"cell_type":"code","source":["# setup estimator object and its parameters\n\n# if you don't use hyperdrive, you will need to specify all flags\n# including the model hyperparameters\nno_hyperdrive_script_params = {\n  '--data-folder': ds.as_mount(),\n  '--n_estimator': 120,\n  '--criterion': 'entropy',\n  '--class_weight': 'balanced_subsample'}\n\nscript_params = {'--data-folder': ds.as_mount()}\n\n# lock package versions to the Databricks environment\n# this is the equivalent of creating a requirements.txt file \npackage_list = [\n  'category-encoders==1.3.0',\n  'numpy==1.15.0',\n  'pandas==0.24.1',\n  'scikit-learn==0.20.2']\n\nest = Estimator(source_directory=Config.SCRIPTS_DIR,\n                script_params=script_params,\n                compute_target=Config.TRAIN_COMPUTE,\n                entry_script='train.py',\n                pip_packages=package_list)\n\n# setup hyperdrive\nparam_sampling = GridParameterSampling({\n  'n_estimator': choice(100, 120),\n  'criterion': choice('entropy', 'gini'),\n  'class_weight': choice('balanced_subsample', 'balanced')})\n\nearly_termination_policy = BanditPolicy(slack_factor=0.1, \n                                        evaluation_interval=1, \n                                        delay_evaluation=4)\n\nhypertune_config = HyperDriveRunConfig(estimator=est,\n                                       hyperparameter_sampling=param_sampling,\n                                       policy = early_termination_policy,\n                                       primary_metric_name='accuracy',\n                                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                       max_total_runs=5,\n                                       max_concurrent_runs=2)\n\nhyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>\nRun(Experiment: pumps-exp1,\nId: pumps-exp1_1550537952873,\nType: hyperdrive,\nStatus: Running)\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["You can monitor the job from Azure Portal or use the Python library azureml-widgets"],"metadata":{}},{"cell_type":"code","source":["# not working need to follow up\nfrom azureml.widgets import RunDetails\nRunDetails(hyperdrive_run).show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Once all the runs are complete, let's select the run that produced the best accuracy"],"metadata":{}},{"cell_type":"code","source":["best_run = hyperdrive_run.get_best_run_by_primary_metric()\nbest_run_metrics = best_run.get_metrics()\nparameter_values = best_run.get_details()['runDefinition']['Arguments']\nprint('Best Run Id: ', best_run.id)\nprint('Accuracy:', best_run_metrics['accuracy'])\nprint('class_weight:', parameter_values[3])\nprint('criterion:', parameter_values[5])\nprint('n_estimators:', parameter_values[7])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Best Run Id:  pumps-exp1_1550537952873_1\nAccuracy: 0.7611111111111111\nclass_weight: gini\ncriterion: 100\nn_estimators: balanced_subsample\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["#### 2.5 REGISTER THE FINAL MODEL\nThe last step in the training script wrote files such as `outputs/rf.pkl` in a directory named outputs in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this directory is automatically uploaded to your workspace. This content appears in the run record in the experiment under your workspace. Hence, the model file as well as the transformers are now also available in your workspace."],"metadata":{}},{"cell_type":"code","source":["# register model as well as input transformers\npumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs/rf.pkl')\npumps_le = best_run.register_model(model_name='pumps_le', model_path='outputs/le.pkl')\npumps_ohc = best_run.register_model(model_name='pumps_ohc', model_path='outputs/ohc.pkl')\npumps_y_le = best_run.register_model(model_name='pumps_y_le', model_path='outputs/y_le.pkl')\n\nprint(pumps_rf.name, pumps_rf.id, pumps_rf.version, sep = '\\t')\nprint(pumps_le.name, pumps_le.id, pumps_le.version, sep = '\\t')\nprint(pumps_ohc.name, pumps_ohc.id, pumps_ohc.version, sep = '\\t')\nprint(pumps_y_le.name, pumps_y_le.id, pumps_y_le.version, sep = '\\t')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">pumps_rf\tpumps_rf:2\t2\npumps_le\tpumps_le:2\t2\npumps_ohc\tpumps_ohc:2\t2\npumps_y_le\tpumps_y_le:2\t2\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["In Part 3 we will take this model we trained and deploy it on a Kubernetes cluster as a web service."],"metadata":{}}],"metadata":{"name":"pumps-azuremls","notebookId":1326659822288831},"nbformat":4,"nbformat_minor":0}
